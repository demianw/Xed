{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWde_EtwQ5i"
      },
      "source": [
        "# Exploration of the Titanic data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smzP4B7HwQ5k"
      },
      "outputs": [],
      "source": [
        "%pip install -q pandas==2.2.2\n",
        "%pip install -q seaborn==0.13.1\n",
        "%pip install -q scikit-learn==1.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNKUn2b1wQ5l"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/demianw/Xed.git\n",
        "%cd Xed/titanic_survival_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbWLzPLpwQ5l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsb_vZjOwQ5l"
      },
      "source": [
        "## Section 1. Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eiwRVquwQ5l"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Load the titanic using `pandas`. It is located in `datasets/titanic.csv`. Using the function `head()` and `info()`, which issues do you identify which need to be solved before to learn a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUg-__bjwQ5l"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../datasets/titanic.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzsFuEecwQ5l"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "- By checking the variable `Survived`, is the dataset balanced? What will be the chance level accuracy?\n",
        "- What variables contain more missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZwqc55xwQ5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l3lHLoEwQ5m"
      },
      "source": [
        "### Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL26we-owQ5m"
      },
      "source": [
        "Using the `pairplot` of `seaborn` on the `Age`, `Pclass`, `Fare`, `Sex`, and `Survived` columns, identify some intuitions regarding the correlation between the survival and the features. Make some plots to confirm your intuition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux7vtBBIwQ5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTPTQmPwQ5m"
      },
      "source": [
        "## Section 2. Predicting survival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu1g_ZPHwQ5m"
      },
      "source": [
        "The titanic dataset is an heterogeneous dataset and it gives the opporunity to show the scikit-learn pipelining features. We will show in this notebook how to make a simple classification pipeline. The aim is to predict or not if a passenger survived the titanic trip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvo1xUy_wQ5m"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../datasets/titanic.csv', index_col='PassengerId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YywzXcT8wQ5m"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEPeJpVwQ5m"
      },
      "source": [
        "First, we need to split the dataset into 2 arrays: the data array and the classification array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPN8AHi9wQ5m"
      },
      "outputs": [],
      "source": [
        "label = data['Survived']\n",
        "data = data.drop(columns='Survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNbqoYE9wQ5m"
      },
      "source": [
        "Because the data type in the titanic dataset, we need to specifically have different preprocessing for the continuous and categorical columns. The `ColumnTransformer` of scikit-learn allows to dispatch different preprocessing depending of the columns. Usually, the categorical variable needs to be encoded while the continuous variable can be standardized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjBSUQQUwQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKGu1CUywQ5n"
      },
      "source": [
        "We are creating three preprocessing:\n",
        "\n",
        "* an ordinal encoding for the sex;\n",
        "* a one hot encoding for the remaining categorical features;\n",
        "* and a standardization for the continuous features.\n",
        "\n",
        "In addition, missing values will be filled up with either the median (for continuous variable) or a constant value (categorical variable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjwOip6swQ5n"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-J87BgCwQ5n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnjK1kAQwQ5n"
      },
      "source": [
        "A logistic regression classifier will be used in which the C parameter will be optimized. We will apply a 5-fold cross-validation scheme to estimate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA6lObswwQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI7oJBsBwQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_test, label_train, label_test = train_test_split(data, label, test_size=.50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhH8_fdXwQ5n"
      },
      "source": [
        "### Compare different classification algorithms to predict survival. Specifically through the learning curve and the prediction quality. You can compare\n",
        "* Logistic Regression, with a parameter C\n",
        "* LinearSVC\n",
        "* RandomForestClassifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGUPLTk6wQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrXsuu2BwQ5n"
      },
      "source": [
        "# Section 3. Model validation and explanatory capabilities. For each model explore fitted model parameters and try to assess their explanatory capabilities\n",
        "\n",
        "Bare in mind that if `pipe` is our processing pipeline composed of a preprocessing step and a LinearSVC step:\n",
        "* `linear_svc = pipe._final_estimator` extracts the tuple (*model name*, *model class*)\n",
        "* `linear_svc` extracts regression class\n",
        "* `linear_svc.coef_` are the coefficients of the regressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dDXtDruwQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspect import permutation_importance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('neurolang')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "270416879d0e6498cbadfae230e6e7fc42b22c2578a4be046b3189f0adba2232"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}