{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmoxa5j0SrS2"
      },
      "outputs": [],
      "source": [
        "%pip install -q pandas==2.2.2\n",
        "%pip install -q seaborn==0.13.1\n",
        "%pip install -q scikit-learn==1.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6gaOgR9SrS3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/demianw/Xed.git\n",
        "%cd Xed/ames_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC0qZNAUSrS3"
      },
      "source": [
        "# Exploring Realstate Sales Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFFbcGEiSrS3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nibPGNQhSrS3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Owba8VSrS3"
      },
      "source": [
        "## 1. Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmR3-CdSrS3"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Load the titanic using `pandas`. It is located in `datasets/ames_housing.csv`. Using the function `head()` and `info()`, which issues do you identify which need to be noted before to learn a machine learning model.\n",
        "\n",
        "The dataset is described in https://www.kaggle.com/datasets/prevek18/ames-housing-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYicQA-uSrS3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../datasets/ames_housing.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MqWbFSzSrS3"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "- Identify the target variable: \"SalePrice\", what's its type? What are its distributionals characteristics?\n",
        "- What variables contain more missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPwmio17SrS3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrHH0Bu6SrS3"
      },
      "source": [
        "## Question 3\n",
        "Split the data into features and target variables.\n",
        "Then, the data into a model selection, sample and a model evaluation sample. Use `sklearn.model_selection.train_test_split`.\n",
        "Use a 20% ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i-OYjtMSrS3",
        "outputId": "a96d8db6-813f-4b96-c4e7-009f16f6dce0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2344,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "target = data[\"SalePrice\"]\n",
        "features = data.drop(columns=\"SalePrice\")\n",
        "\n",
        "selection_features, evaluation_features, selection_target, evaluation_target = train_test_split(\n",
        "    features, target, test_size=.2\n",
        ")\n",
        "selection_target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdKtm8BaSrS4"
      },
      "source": [
        "## Question 4\n",
        "Extract the columns with numerical data using `selection_features.select_dtypes(\"number\")`. Examine their distributions, through histograms. What issues do you identify? Then use  `selection_features.select_dtypes(\"number\")` and seaborn's `sns.countplot` to analyze the string variables. Identify data types and issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8P8SamVSrS4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Yc5U4sSrS4"
      },
      "source": [
        "# Section 2: Implement a linear regressor using only the numerical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZVsZLGmSrS4"
      },
      "source": [
        "### Question 1\n",
        "Use a Column Transformer to _just select_ the numerical variables. Build a linear regressor using `sklearn.linear.LinearRegressor\n",
        "\n",
        "For this we will\n",
        "* build the column transformer\n",
        "* build the machine learning pipeline\n",
        "* evaluate it trough cross-validation (using `cross_vals_score`)\n",
        "\n",
        "Does it work? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjbY94gTSrS4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpo_XToASrS4"
      },
      "source": [
        "### Question 2\n",
        "Fix the previous issue using, first dropping the problemating rows, then using the `SimpleImputer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqJBV-8CSrS4"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21zVbxy1SrS4"
      },
      "source": [
        "### Question 3\n",
        "Now plot the evolution of mean and standard deviations for test sample sizes of 05%, 10%, 20%, 25%, 30%. What do you conclude?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5PAxZssSrS4"
      },
      "source": [
        "### Question 4\n",
        "Use `sklearn.model_selection.learning_curve` to study the learning curve https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp3U7iyNSrS4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cxlbDUbSrS4"
      },
      "source": [
        "### Question 5\n",
        "Are there corrleations between the features? Explore it through the correlation matrix, and the `sns.pairplot` plotting tool from seaborn (warning, if you plot all variables together it might be slow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g7Y8QJRSrS4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epzj0dmBSrS4"
      },
      "source": [
        "### Question 6\n",
        "Can we use this correlation to improve the learning curve? This is regularization, let's try ridge regression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
        "\n",
        "Plot the learning curve and compare it with the plain linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUHHf19uSrS4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUFF1E55SrS4"
      },
      "source": [
        "### Question 6.1\n",
        "How did you pick your regularization parameter? Use a grid search now. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WamaclDSrS4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STgB-PPmSrS4"
      },
      "source": [
        "### Question 7\n",
        "Do we need all features? Repeat the previous analysis from Question 6 but with the Lasso which enforces sparsity https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOt4ISRQSrS4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxyj878WSrS4"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Now we will repeat the same analysis but with the categorical variables. For which we will use the `OneHotEncoder` and the `OrdinalEncoder`\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        "\n",
        "and combine them in the preprocessing pipeline in Section 2, Questions 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbf34MwGSrS5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KkNyQgQSrS5"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "Non linearity! Now use the RandomForestRegressor https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html to fit and predict the data. The two hyper-parameters that you will use are\n",
        "\n",
        "* n_estimators : with a default of 100 which deals with the uncertainty in the data/algorithm relationship.\n",
        "* max_depth : with a no limit as a default which deals with the granularity of the solution.\n",
        "\n",
        "Use a Grid search cross validation to set the two parameters. Plot the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9Vs4MhSrS5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYia91EGSrS5"
      },
      "source": [
        "## Question 10\n",
        "\n",
        "We will now use the data to obtain\n",
        "Use permutation feature importance to assess which are the most important features in predicting house pricing https://scikit-learn.org/stable/modules/permutation_importance.html\n",
        "\n",
        "Compare these importances across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLWDbE0RSrS5"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDJUIozZSrS5"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "Pick one of the estimators. Use cross_val_predict to evaluate the quality of the prediction in different cases.\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\n",
        "\n",
        "Cross-val predict will give you for each element in the target, a prediction. Produce a scatterplot between target and prediction, use the trained model and the predictive importance to find the most explanatory variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4wxAKnVSrS5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QBgYERaSrS5"
      },
      "source": [
        "# Section 3: Analyzing our phenomenon.\n",
        "\n",
        "Now that we have picked one model:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('neurolang')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "270416879d0e6498cbadfae230e6e7fc42b22c2578a4be046b3189f0adba2232"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}